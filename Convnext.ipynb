{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c15c1c7f-c830-4acf-922d-f26b832d0664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.0+cu121 with CUDA 1201 (you have 2.0.1+cu117)\n",
      "    Python  3.9.18 (you have 3.9.17)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from transformers import *\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torchinfo import summary\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50876b6-87da-467d-8ab7-2379d224e3e1",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e4f22a-4665-449e-a9a1-79f891e6fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(examples):\n",
    "  inputs = image_processor([img.convert(\"RGB\") for img in examples[\"image\"]], return_tensors=\"pt\")\n",
    "  inputs[\"labels\"] = examples[\"label\"]\n",
    "  return inputs\n",
    "\n",
    "def collate_fn(batch):\n",
    "  return {\n",
    "      \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
    "      \"labels\": torch.tensor([x[\"labels\"] for x in batch]),\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ed81720-8a58-45b2-8f19-37d29e2a7cf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at /home/moose/.cache/huggingface/hub/models--facebook--convnext-base-224-22k/snapshots/a801e9ad0e52947a4583a14696afbe20decbf89e/preprocessor_config.json\n",
      "size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}), got 224. Converted to {'shortest_edge': 224}.\n",
      "Image processor ConvNextImageProcessor {\n",
      "  \"crop_pct\": 0.875,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"ConvNextFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_processor_type\": \"ConvNextImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 224\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c51e9110cb451abb6216ba35a781bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/5840 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"facebook/convnext-base-224-22k\"\n",
    "batch_size = 16\n",
    "cpu_count=multiprocessing.cpu_count()\n",
    "\n",
    "image_processor = ConvNextImageProcessor.from_pretrained(\"facebook/convnext-base-224-22k\")\n",
    "\n",
    "train_ds= load_dataset('./chest_xray/data')\n",
    "train_ds = train_ds[\"train\"].train_test_split(test_size=0.25) \n",
    "\n",
    "labels = train_ds[\"train\"].features[\"label\"].names\n",
    "dataset = train_ds.with_transform(transform)\n",
    "\n",
    "\n",
    "train_dataset_loader = torch.utils.data.DataLoader(dataset[\"train\"], collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
    "valid_dataset_loader = torch.utils.data.DataLoader(dataset[\"test\"], collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840f4076-1faa-4ccc-b98f-c652ece789a6",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cac33c31-2a45-4e66-b036-5a563206f18c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/moose/.cache/huggingface/hub/models--facebook--convnext-base-224-22k/snapshots/a801e9ad0e52947a4583a14696afbe20decbf89e/config.json\n",
      "Model config ConvNextConfig {\n",
      "  \"architectures\": [\n",
      "    \"ConvNextForImageClassification\"\n",
      "  ],\n",
      "  \"depths\": [\n",
      "    3,\n",
      "    3,\n",
      "    27,\n",
      "    3\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_sizes\": [\n",
      "    128,\n",
      "    256,\n",
      "    512,\n",
      "    1024\n",
      "  ],\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NORMAL\",\n",
      "    \"1\": \"PNEUMONIA\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NORMAL\": \"0\",\n",
      "    \"PNEUMONIA\": \"1\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"layer_scale_init_value\": 1e-06,\n",
      "  \"model_type\": \"convnext\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_stages\": 4,\n",
      "  \"out_features\": [\n",
      "    \"stage4\"\n",
      "  ],\n",
      "  \"out_indices\": [\n",
      "    4\n",
      "  ],\n",
      "  \"patch_size\": 4,\n",
      "  \"stage_names\": [\n",
      "    \"stem\",\n",
      "    \"stage1\",\n",
      "    \"stage2\",\n",
      "    \"stage3\",\n",
      "    \"stage4\"\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/moose/.cache/huggingface/hub/models--facebook--convnext-base-224-22k/snapshots/a801e9ad0e52947a4583a14696afbe20decbf89e/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ConvNextForImageClassification.\n",
      "\n",
      "Some weights of ConvNextForImageClassification were not initialized from the model checkpoint at facebook/convnext-base-224-22k and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([21841, 1024]) in the checkpoint and torch.Size([2, 1024]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model =  ConvNextForImageClassification.from_pretrained(model_name,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)},\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fe718b-8a51-49cb-a72c-5735ac890eb1",
   "metadata": {},
   "source": [
    "### Optimizer and accuracy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb327302-863a-4cee-918c-4a530c39a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "log_dir = \"./convnext_base_224_tensorboard/\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "def accuracy_fn(logits,true):\n",
    "    return torch.eq(torch.argmax(torch.softmax(logits,dim=1),dim=1).squeeze(),true).sum().item()/len(logits)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "920d9786-0c53-4a6b-a1a6-73b46201fdff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3664a3af77cc42a5bc76851bc48cecdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a401ed0a05224eba9043fd1c64eb249f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0918797651541549b6cbceeac737aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./convnext-base-224/checkpoint-1/config.json\n",
      "Model weights saved in ./convnext-base-224/checkpoint-1/pytorch_model.bin\n",
      "Image processor saved in ./convnext-base4-224/checkpoint-1/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train Accuracy: 0.9226733576642335  Train Loss: 0.18793842161794866  Test Accuracy: 0.970108695652174  Test Loss: 0.08981894205688783\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37903e7a3f5a44b3a2c6961f3e251b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3152e0d3497d45b7b2676e3469c3ff2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./convnext-base-224/checkpoint-2/config.json\n",
      "Model weights saved in ./convnext-base-224/checkpoint-2/pytorch_model.bin\n",
      "Image processor saved in ./convnext-base4-224/checkpoint-2/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "Train Accuracy: 0.9765054744525548  Train Loss: 0.06987172689112107  Test Accuracy: 0.9748641304347826  Test Loss: 0.0732910830309898\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb45429496074f228058fa511e2331b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7923dce1ce4e47895ce22de71ce01c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./convnext-base-224/checkpoint-3/config.json\n",
      "Model weights saved in ./convnext-base-224/checkpoint-3/pytorch_model.bin\n",
      "Image processor saved in ./convnext-base4-224/checkpoint-3/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "Train Accuracy: 0.9904197080291971  Train Loss: 0.029527654149697033  Test Accuracy: 0.9653532608695652  Test Loss: 0.09188038663705811\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b28efb34b64013912b4dd861c22a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca97e3873fb4c71b989ee4ee97d5615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./convnext-base-224/checkpoint-4/config.json\n",
      "Model weights saved in ./convnext-base-224/checkpoint-4/pytorch_model.bin\n",
      "Image processor saved in ./convnext-base4-224/checkpoint-4/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "Train Accuracy: 0.9974908759124088  Train Loss: 0.010013097570444301  Test Accuracy: 0.9680706521739131  Test Loss: 0.11775643122002846\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aea97fd27e349e7af50505caa27caa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43910da643d14fe3914b52e2e1d818bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./convnext-base-224/checkpoint-5/config.json\n",
      "Model weights saved in ./convnext-base-224/checkpoint-5/pytorch_model.bin\n",
      "Image processor saved in ./convnext-base4-224/checkpoint-5/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "Train Accuracy: 0.9942974452554745  Train Loss: 0.017260089394010615  Test Accuracy: 0.9802989130434783  Test Loss: 0.07666135594706208\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0b176c902c49f99aacc6831530b803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508b862a812a4b41948e51f6bd1567da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./convnext-base-224/checkpoint-6/config.json\n",
      "Model weights saved in ./convnext-base-224/checkpoint-6/pytorch_model.bin\n",
      "Image processor saved in ./convnext-base4-224/checkpoint-6/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "Train Accuracy: 0.9995437956204379  Train Loss: 0.00235131768068727  Test Accuracy: 0.9782608695652174  Test Loss: 0.08411201592958684\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d65202862a40d5a7f4aede4dc948df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2deadd10271944839e21d264b5de301c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./convnext-base-224/checkpoint-7/config.json\n",
      "Model weights saved in ./convnext-base-224/checkpoint-7/pytorch_model.bin\n",
      "Image processor saved in ./convnext-base4-224/checkpoint-7/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\n",
      "Train Accuracy: 0.9995437956204379  Train Loss: 0.0012384355876262955  Test Accuracy: 0.9796195652173914  Test Loss: 0.08697426701726964\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb17c92bf1c4d5681f24817df26a401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5111b22cdc514a4788af4c215f79597e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./convnext-base-224/checkpoint-8/config.json\n",
      "Model weights saved in ./convnext-base-224/checkpoint-8/pytorch_model.bin\n",
      "Image processor saved in ./convnext-base4-224/checkpoint-8/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:\n",
      "Train Accuracy: 1.0  Train Loss: 0.0006276024893989674  Test Accuracy: 0.9809782608695652  Test Loss: 0.0900637021779561\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c8154525b04aa2a716ab6aacf41754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a64905c46b14013b94e6587c2691e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./convnext-base-224/checkpoint-9/config.json\n",
      "Model weights saved in ./convnext-base-224/checkpoint-9/pytorch_model.bin\n",
      "Image processor saved in ./convnext-base4-224/checkpoint-9/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:\n",
      "Train Accuracy: 1.0  Train Loss: 0.0003050954693095184  Test Accuracy: 0.9816576086956522  Test Loss: 0.09211874081341628\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f06eae2ff145e4a708c2aaf806e4e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04dedf52f314270abbf52448c4bfd33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./convnext-base-224/checkpoint-10/config.json\n",
      "Model weights saved in ./convnext-base-224/checkpoint-10/pytorch_model.bin\n",
      "Image processor saved in ./convnext-base4-224/checkpoint-10/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:\n",
      "Train Accuracy: 1.0  Train Loss: 0.00019403937994534351  Test Accuracy: 0.9816576086956522  Test Loss: 0.09465228093714635\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "\n",
    "train_accuracy=[]\n",
    "test_accuracy=[]\n",
    "train_loss=[]\n",
    "test_loss=[]\n",
    "\n",
    "for i in tqdm(range(epochs)):\n",
    "    print(\"Training:\")\n",
    "    model.train()\n",
    "    \n",
    "    #Defining accuracy and loss for train and test data\n",
    "    temp_train_accuracy=[]\n",
    "    temp_test_accuracy=[]\n",
    "    temp_train_loss=[]\n",
    "    temp_test_loss=[]\n",
    "\n",
    "    net_train_accuracy=0\n",
    "    net_test_accuracy=0\n",
    "    net_train_loss=0\n",
    "    net_test_loss=0\n",
    "    \n",
    "    with tqdm(total=len(train_dataset_loader)) as pbar:\n",
    "        for batch in train_dataset_loader:\n",
    "                x=batch[\"pixel_values\"].to(device)\n",
    "                y=batch[\"labels\"].to(device)\n",
    "    \n",
    "                #Calculating model output\n",
    "                result=model(pixel_values=x,labels=y)\n",
    "                logits=result.logits\n",
    "    \n",
    "                #Reseting any old gradient values\n",
    "                optimizer.zero_grad()\n",
    "                #loss=loss_fn(logits.squeeze(),y.type(torch.float32))\n",
    "                loss=result.loss\n",
    "\n",
    "            \n",
    "                #Track of metrics        \n",
    "                accuracy_train=accuracy_fn(logits,y)\n",
    "                temp_train_accuracy.append(accuracy_train)\n",
    "                temp_train_loss.append(loss.item())\n",
    "    \n",
    "                #Back Propogation\n",
    "                loss.backward()\n",
    "            \n",
    "                #Update Parameters\n",
    "                optimizer.step()\n",
    "            \n",
    "                #Progress Bar Update\n",
    "                pbar.update(1)\n",
    "        pbar.close()\n",
    "    #Tensorboard & Metrics for the dataset\n",
    "    net_train_accuracy=sum(temp_train_accuracy)/len(temp_train_accuracy)\n",
    "    net_train_loss=sum(temp_train_loss)/len(temp_train_loss)\n",
    "    train_accuracy.append(net_train_accuracy)\n",
    "    train_loss.append(net_train_loss)\n",
    "    writer.add_scalar(\"Train Accuracy\",net_train_accuracy,i)\n",
    "    writer.add_scalar(\"Train Loss\",net_train_loss,i)\n",
    "\n",
    "    #Evaluation\n",
    "    print(\"Testing:\")\n",
    "    model.eval()\n",
    "\n",
    "    with tqdm(total=len(valid_dataset_loader)) as pbar2:\n",
    "        for batch in valid_dataset_loader:\n",
    "            x=batch[\"pixel_values\"].to(device)\n",
    "            y=batch[\"labels\"].to(device)\n",
    "            \n",
    "            #Setting inference mode\n",
    "            with torch.inference_mode():\n",
    "                result=model(pixel_values=x,labels=y)\n",
    "                logits=result.logits\n",
    "                #loss=loss_fn(logits.squeeze(),y.type(torch.float32))\n",
    "                loss=result.loss\n",
    "\n",
    "                #Track of metrics\n",
    "                accuracy_test=accuracy_fn(logits,y)\n",
    "                temp_test_accuracy.append(accuracy_test)\n",
    "                temp_test_loss.append(loss.item())\n",
    "\n",
    "                #Progress Bar Update\n",
    "                pbar2.update(1)\n",
    "        pbar2.close()\n",
    "\n",
    "    #Tensorboard & Metrics for the dataset\n",
    "    net_test_accuracy=sum(temp_test_accuracy)/len(temp_test_accuracy)\n",
    "    net_test_loss=sum(temp_test_loss)/len(temp_test_loss)\n",
    "    test_accuracy.append(net_test_accuracy)\n",
    "    test_loss.append(net_test_loss)\n",
    "    writer.add_scalar(\"Test Accuracy\",net_test_accuracy,i)\n",
    "    writer.add_scalar(\"Test Loss\",net_test_loss,i)\n",
    "\n",
    "    #Saving the model\n",
    "    model.save_pretrained(f\"./convnext-base-224/checkpoint-{i+1}\")\n",
    "    image_processor.save_pretrained(f\"./convnext-base4-224/checkpoint-{i+1}\")\n",
    "\n",
    "    print(f\"Epoch {i+1}:\\nTrain Accuracy: {net_train_accuracy}  Train Loss: {net_train_loss}  Test Accuracy: {net_test_accuracy}  Test Loss: {net_test_loss}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

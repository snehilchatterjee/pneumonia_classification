{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c15c1c7f-c830-4acf-922d-f26b832d0664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.0+cu121 with CUDA 1201 (you have 2.0.1+cu117)\n",
      "    Python  3.9.18 (you have 3.9.17)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from transformers import *\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torchinfo import summary\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50876b6-87da-467d-8ab7-2379d224e3e1",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e4f22a-4665-449e-a9a1-79f891e6fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(examples):\n",
    "  inputs = image_processor([img.convert(\"RGB\") for img in examples[\"image\"]], return_tensors=\"pt\")\n",
    "  inputs[\"labels\"] = examples[\"label\"]\n",
    "  return inputs\n",
    "\n",
    "def collate_fn(batch):\n",
    "  return {\n",
    "      \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
    "      \"labels\": torch.tensor([x[\"labels\"] for x in batch]),\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ed81720-8a58-45b2-8f19-37d29e2a7cf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at /home/moose/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/68dc76680a5bf3bdf670669f3025dc9be2e30781/preprocessor_config.json\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
      "Image processor ViTImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e8a75b818f448ca2ca92862bd004f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/5840 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"microsoft/swin-base-patch4-window7-224-in22k\"\n",
    "batch_size = 16\n",
    "cpu_count=multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "train_ds= load_dataset('./chest_xray/data')\n",
    "train_ds = train_ds[\"train\"].train_test_split(test_size=0.25) \n",
    "\n",
    "labels = train_ds[\"train\"].features[\"label\"].names\n",
    "dataset = train_ds.with_transform(transform)\n",
    "\n",
    "\n",
    "train_dataset_loader = torch.utils.data.DataLoader(dataset[\"train\"], collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
    "valid_dataset_loader = torch.utils.data.DataLoader(dataset[\"test\"], collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840f4076-1faa-4ccc-b98f-c652ece789a6",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cac33c31-2a45-4e66-b036-5a563206f18c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/moose/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/68dc76680a5bf3bdf670669f3025dc9be2e30781/config.json\n",
      "Model config SwinConfig {\n",
      "  \"_name_or_path\": \"microsoft/swin-base-patch4-window7-224-in22k\",\n",
      "  \"architectures\": [\n",
      "    \"SwinForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"depths\": [\n",
      "    2,\n",
      "    2,\n",
      "    18,\n",
      "    2\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"embed_dim\": 128,\n",
      "  \"encoder_stride\": 32,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NORMAL\",\n",
      "    \"1\": \"PNEUMONIA\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NORMAL\": \"0\",\n",
      "    \"PNEUMONIA\": \"1\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mlp_ratio\": 4.0,\n",
      "  \"model_type\": \"swin\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_heads\": [\n",
      "    4,\n",
      "    8,\n",
      "    16,\n",
      "    32\n",
      "  ],\n",
      "  \"num_layers\": 4,\n",
      "  \"out_features\": [\n",
      "    \"stage4\"\n",
      "  ],\n",
      "  \"out_indices\": [\n",
      "    4\n",
      "  ],\n",
      "  \"patch_size\": 4,\n",
      "  \"path_norm\": true,\n",
      "  \"qkv_bias\": true,\n",
      "  \"stage_names\": [\n",
      "    \"stem\",\n",
      "    \"stage1\",\n",
      "    \"stage2\",\n",
      "    \"stage3\",\n",
      "    \"stage4\"\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.33.3\",\n",
      "  \"use_absolute_embeddings\": false,\n",
      "  \"window_size\": 7\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/moose/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/68dc76680a5bf3bdf670669f3025dc9be2e30781/model.safetensors\n",
      "All model checkpoint weights were used when initializing SwinForImageClassification.\n",
      "\n",
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224-in22k and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([21841, 1024]) in the checkpoint and torch.Size([2, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = SwinForImageClassification.from_pretrained(model_name,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)},\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fe718b-8a51-49cb-a72c-5735ac890eb1",
   "metadata": {},
   "source": [
    "### Optimizer and accuracy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb327302-863a-4cee-918c-4a530c39a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "log_dir = \"./swin_base4_224_tensorboard/\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "def accuracy_fn(logits,true):\n",
    "    return torch.eq(torch.argmax(torch.softmax(logits,dim=1),dim=1).squeeze(),true).sum().item()/len(logits)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "920d9786-0c53-4a6b-a1a6-73b46201fdff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36227bde0d6c4726bf2e2569c6b99a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda28cd18d854b4abcceec8caab5146f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4febb19a8448e2be57279c7d4e4b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./swin-base4-224/checkpoint-1/config.json\n",
      "Model weights saved in ./swin-base4-224/checkpoint-1/pytorch_model.bin\n",
      "Image processor saved in ./swin-base4-224/checkpoint-1/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train Accuracy: 0.926094890510949  Train Loss: 0.18993109787379248  Test Accuracy: 0.9721467391304348  Test Loss: 0.08757042326792346\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc5071a66664000a8a18d47f53b2d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ed641755154cb5954ca3f2fcf1bb47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./swin-base4-224/checkpoint-2/config.json\n",
      "Model weights saved in ./swin-base4-224/checkpoint-2/pytorch_model.bin\n",
      "Image processor saved in ./swin-base4-224/checkpoint-2/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "Train Accuracy: 0.9710310218978102  Train Loss: 0.07973084214991502  Test Accuracy: 0.967391304347826  Test Loss: 0.09401738132684208\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484cd5cde19d436faaaaf8e68bb27c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6fa48d0f2704271bf218ffaeea46786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./swin-base4-224/checkpoint-3/config.json\n",
      "Model weights saved in ./swin-base4-224/checkpoint-3/pytorch_model.bin\n",
      "Image processor saved in ./swin-base4-224/checkpoint-3/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "Train Accuracy: 0.9819799270072993  Train Loss: 0.05504873073251959  Test Accuracy: 0.9694293478260869  Test Loss: 0.08235352170721973\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed001c5912a84dfaaee005ca0e7776e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1855f4cf579c40d3b3847ca119d22309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./swin-base4-224/checkpoint-4/config.json\n",
      "Model weights saved in ./swin-base4-224/checkpoint-4/pytorch_model.bin\n",
      "Image processor saved in ./swin-base4-224/checkpoint-4/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "Train Accuracy: 0.9844890510948905  Train Loss: 0.044711291423448984  Test Accuracy: 0.9680706521739131  Test Loss: 0.09452761938914393\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d05ab9a0074a0c8a931dacdfe51a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dadd50b9f404428b6e1867a19a9a289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./swin-base4-224/checkpoint-5/config.json\n",
      "Model weights saved in ./swin-base4-224/checkpoint-5/pytorch_model.bin\n",
      "Image processor saved in ./swin-base4-224/checkpoint-5/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "Train Accuracy: 0.9904197080291971  Train Loss: 0.029604119332706944  Test Accuracy: 0.9714673913043478  Test Loss: 0.07809242875136198\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fab77e47d2f4167838fe21c4b78fcee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d2e656ef6740328db8b4874483d53d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./swin-base4-224/checkpoint-6/config.json\n",
      "Model weights saved in ./swin-base4-224/checkpoint-6/pytorch_model.bin\n",
      "Image processor saved in ./swin-base4-224/checkpoint-6/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "Train Accuracy: 0.9942974452554745  Train Loss: 0.02084806887215185  Test Accuracy: 0.9755434782608695  Test Loss: 0.08286913673221177\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738e5a2399f540c188126f3ec6a24e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a080b1a02b9f4d0a97214cbbed3df2e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./swin-base4-224/checkpoint-7/config.json\n",
      "Model weights saved in ./swin-base4-224/checkpoint-7/pytorch_model.bin\n",
      "Image processor saved in ./swin-base4-224/checkpoint-7/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\n",
      "Train Accuracy: 0.9970346715328468  Train Loss: 0.009450791384146891  Test Accuracy: 0.970108695652174  Test Loss: 0.0925728987206679\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b3f941c8b04ff3a28e2e0ebe5c894d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17cda39d3c748eeb39091fcf243fffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./swin-base4-224/checkpoint-8/config.json\n",
      "Model weights saved in ./swin-base4-224/checkpoint-8/pytorch_model.bin\n",
      "Image processor saved in ./swin-base4-224/checkpoint-8/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:\n",
      "Train Accuracy: 0.9949817518248175  Train Loss: 0.011671174737648803  Test Accuracy: 0.9762228260869565  Test Loss: 0.12534871347855622\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f04b14430646d490bb979d71c07021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a7527e5bc440f09a36f3dd3aa5c642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./swin-base4-224/checkpoint-9/config.json\n",
      "Model weights saved in ./swin-base4-224/checkpoint-9/pytorch_model.bin\n",
      "Image processor saved in ./swin-base4-224/checkpoint-9/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:\n",
      "Train Accuracy: 0.9963503649635036  Train Loss: 0.011648896437525326  Test Accuracy: 0.9748641304347826  Test Loss: 0.11324954303010956\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1a7211fcb54fc295698e4726363b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de556819ad324b839fa4594965ceba58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./swin-base4-224/checkpoint-10/config.json\n",
      "Model weights saved in ./swin-base4-224/checkpoint-10/pytorch_model.bin\n",
      "Image processor saved in ./swin-base4-224/checkpoint-10/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:\n",
      "Train Accuracy: 0.9978710462287105  Train Loss: 0.0067393360615200315  Test Accuracy: 0.9769021739130435  Test Loss: 0.10731639343818307\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "\n",
    "train_accuracy=[]\n",
    "test_accuracy=[]\n",
    "train_loss=[]\n",
    "test_loss=[]\n",
    "\n",
    "for i in tqdm(range(epochs)):\n",
    "    print(\"Training:\")\n",
    "    model.train()\n",
    "    \n",
    "    #Defining accuracy and loss for train and test data\n",
    "    temp_train_accuracy=[]\n",
    "    temp_test_accuracy=[]\n",
    "    temp_train_loss=[]\n",
    "    temp_test_loss=[]\n",
    "\n",
    "    net_train_accuracy=0\n",
    "    net_test_accuracy=0\n",
    "    net_train_loss=0\n",
    "    net_test_loss=0\n",
    "    \n",
    "    with tqdm(total=len(train_dataset_loader)) as pbar:\n",
    "        for batch in train_dataset_loader:\n",
    "                x=batch[\"pixel_values\"].to(device)\n",
    "                y=batch[\"labels\"].to(device)\n",
    "    \n",
    "                #Calculating model output\n",
    "                result=model(pixel_values=x,labels=y)\n",
    "                logits=result.logits\n",
    "    \n",
    "                #Reseting any old gradient values\n",
    "                optimizer.zero_grad()\n",
    "                #loss=loss_fn(logits.squeeze(),y.type(torch.float32))\n",
    "                loss=result.loss\n",
    "\n",
    "            \n",
    "                #Track of metrics        \n",
    "                accuracy_train=accuracy_fn(logits,y)\n",
    "                temp_train_accuracy.append(accuracy_train)\n",
    "                temp_train_loss.append(loss.item())\n",
    "    \n",
    "                #Back Propogation\n",
    "                loss.backward()\n",
    "            \n",
    "                #Update Parameters\n",
    "                optimizer.step()\n",
    "            \n",
    "                #Progress Bar Update\n",
    "                pbar.update(1)\n",
    "        pbar.close()\n",
    "    #Tensorboard & Metrics for the dataset\n",
    "    net_train_accuracy=sum(temp_train_accuracy)/len(temp_train_accuracy)\n",
    "    net_train_loss=sum(temp_train_loss)/len(temp_train_loss)\n",
    "    train_accuracy.append(net_train_accuracy)\n",
    "    train_loss.append(net_train_loss)\n",
    "    writer.add_scalar(\"Train Accuracy\",net_train_accuracy,i)\n",
    "    writer.add_scalar(\"Train Loss\",net_train_loss,i)\n",
    "\n",
    "    #Evaluation\n",
    "    print(\"Testing:\")\n",
    "    model.eval()\n",
    "\n",
    "    with tqdm(total=len(valid_dataset_loader)) as pbar2:\n",
    "        for batch in valid_dataset_loader:\n",
    "            x=batch[\"pixel_values\"].to(device)\n",
    "            y=batch[\"labels\"].to(device)\n",
    "            \n",
    "            #Setting inference mode\n",
    "            with torch.inference_mode():\n",
    "                result=model(pixel_values=x,labels=y)\n",
    "                logits=result.logits\n",
    "                #loss=loss_fn(logits.squeeze(),y.type(torch.float32))\n",
    "                loss=result.loss\n",
    "\n",
    "                #Track of metrics\n",
    "                accuracy_test=accuracy_fn(logits,y)\n",
    "                temp_test_accuracy.append(accuracy_test)\n",
    "                temp_test_loss.append(loss.item())\n",
    "\n",
    "                #Progress Bar Update\n",
    "                pbar2.update(1)\n",
    "        pbar2.close()\n",
    "\n",
    "    #Tensorboard & Metrics for the dataset\n",
    "    net_test_accuracy=sum(temp_test_accuracy)/len(temp_test_accuracy)\n",
    "    net_test_loss=sum(temp_test_loss)/len(temp_test_loss)\n",
    "    test_accuracy.append(net_test_accuracy)\n",
    "    test_loss.append(net_test_loss)\n",
    "    writer.add_scalar(\"Test Accuracy\",net_test_accuracy,i)\n",
    "    writer.add_scalar(\"Test Loss\",net_test_loss,i)\n",
    "\n",
    "    #Saving the model\n",
    "    model.save_pretrained(f\"./swin-base4-224/checkpoint-{i+1}\")\n",
    "    image_processor.save_pretrained(f\"./swin-base4-224/checkpoint-{i+1}\")\n",
    "\n",
    "    print(f\"Epoch {i+1}:\\nTrain Accuracy: {net_train_accuracy}  Train Loss: {net_train_loss}  Test Accuracy: {net_test_accuracy}  Test Loss: {net_test_loss}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

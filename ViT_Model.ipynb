{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c15c1c7f-c830-4acf-922d-f26b832d0664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.0+cu121 with CUDA 1201 (you have 2.0.1+cu117)\n",
      "    Python  3.9.18 (you have 3.9.17)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from transformers import *\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torchinfo import summary\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50876b6-87da-467d-8ab7-2379d224e3e1",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e4f22a-4665-449e-a9a1-79f891e6fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(examples):\n",
    "  inputs = image_processor([img.convert(\"RGB\") for img in examples[\"image\"]], return_tensors=\"pt\")\n",
    "  inputs[\"labels\"] = examples[\"label\"]\n",
    "  return inputs\n",
    "\n",
    "def collate_fn(batch):\n",
    "  return {\n",
    "      \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
    "      \"labels\": torch.tensor([x[\"labels\"] for x in batch]),\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ed81720-8a58-45b2-8f19-37d29e2a7cf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at /home/moose/.cache/huggingface/hub/models--google--vit-base-patch16-384/snapshots/2960116e809e2fca84146dbb240289aee7db4827/preprocessor_config.json\n",
      "size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}), got 384. Converted to {'height': 384, 'width': 384}.\n",
      "Image processor ViTImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 384,\n",
      "    \"width\": 384\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec3073e11bc440b9b423935e69a494b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/5840 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"google/vit-base-patch16-384\"\n",
    "batch_size = 6\n",
    "cpu_count=multiprocessing.cpu_count()\n",
    "\n",
    "image_processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "train_ds= load_dataset('./chest_xray/data')\n",
    "train_ds = train_ds[\"train\"].train_test_split(test_size=0.25) \n",
    "\n",
    "\n",
    "labels = train_ds[\"train\"].features[\"label\"].names\n",
    "dataset = train_ds.with_transform(transform)\n",
    "\n",
    "\n",
    "train_dataset_loader = torch.utils.data.DataLoader(dataset[\"train\"], collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
    "valid_dataset_loader = torch.utils.data.DataLoader(dataset[\"test\"], collate_fn=collate_fn, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840f4076-1faa-4ccc-b98f-c652ece789a6",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cac33c31-2a45-4e66-b036-5a563206f18c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/moose/.cache/huggingface/hub/models--google--vit-base-patch16-384/snapshots/2960116e809e2fca84146dbb240289aee7db4827/config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-base-patch16-384\",\n",
      "  \"architectures\": [\n",
      "    \"ViTForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NORMAL\",\n",
      "    \"1\": \"PNEUMONIA\"\n",
      "  },\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"NORMAL\": \"0\",\n",
      "    \"PNEUMONIA\": \"1\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/moose/.cache/huggingface/hub/models--google--vit-base-patch16-384/snapshots/2960116e809e2fca84146dbb240289aee7db4827/model.safetensors\n",
      "All model checkpoint weights were used when initializing ViTForImageClassification.\n",
      "\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-384 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)},\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fe718b-8a51-49cb-a72c-5735ac890eb1",
   "metadata": {},
   "source": [
    "### Optimizer and accuracy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb327302-863a-4cee-918c-4a530c39a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "log_dir = \"./ViT_base_384_tensorboard/\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "def accuracy_fn(logits,true):\n",
    "    return torch.eq(torch.argmax(torch.softmax(logits,dim=1),dim=1).squeeze(),true).sum().item()/len(logits)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "920d9786-0c53-4a6b-a1a6-73b46201fdff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cbedf2e4a14d00b518cc3fde1b84ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a605c27c62e4401a85b1533e0d385885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc63e5ab3d44b8a904a107b7008a24c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./vit-base-384/checkpoint-1/config.json\n",
      "Model weights saved in ./vit-base-384/checkpoint-1/pytorch_model.bin\n",
      "Image processor saved in ./vit-base-384/checkpoint-1/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train Accuracy: 0.9410958904109589  Train Loss: 0.14798766325550053  Test Accuracy: 0.9774590163934429  Test Loss: 0.0654667591233356\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b850e01358741548a0b610ad2c0bf45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89aae5ae189d476d9e996662ed0a5574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./vit-base-384/checkpoint-2/config.json\n",
      "Model weights saved in ./vit-base-384/checkpoint-2/pytorch_model.bin\n",
      "Image processor saved in ./vit-base-384/checkpoint-2/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "Train Accuracy: 0.9780821917808227  Train Loss: 0.06396807534839441  Test Accuracy: 0.9760928961748637  Test Loss: 0.05894467266601869\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73514bb2d74e4349a32ec2afd8159840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bfccc58a9ff4af6be1609909873d454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./vit-base-384/checkpoint-3/config.json\n",
      "Model weights saved in ./vit-base-384/checkpoint-3/pytorch_model.bin\n",
      "Image processor saved in ./vit-base-384/checkpoint-3/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "Train Accuracy: 0.9913242009132426  Train Loss: 0.0271337835496325  Test Accuracy: 0.9747267759562844  Test Loss: 0.07490119638395464\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a324bfab1ea84f41a266f5dc3838bd22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3045e1eedd04ffa961afef2c90a8217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./vit-base-384/checkpoint-4/config.json\n",
      "Model weights saved in ./vit-base-384/checkpoint-4/pytorch_model.bin\n",
      "Image processor saved in ./vit-base-384/checkpoint-4/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "Train Accuracy: 0.9968036529680367  Train Loss: 0.011432552172646703  Test Accuracy: 0.9760928961748638  Test Loss: 0.07711402319755059\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67eb6ad5fa0541419b04837ae692ce34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f1bfbccb8442468a8b089dfb814bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./vit-base-384/checkpoint-5/config.json\n",
      "Model weights saved in ./vit-base-384/checkpoint-5/pytorch_model.bin\n",
      "Image processor saved in ./vit-base-384/checkpoint-5/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "Train Accuracy: 0.9956621004566215  Train Loss: 0.012802572600268009  Test Accuracy: 0.9781420765027327  Test Loss: 0.07533510096330798\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bd27ac461049c7801d6ef16ab2935a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5557f2e1aad544f9be7ecd14ea10ecb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./vit-base-384/checkpoint-6/config.json\n",
      "Model weights saved in ./vit-base-384/checkpoint-6/pytorch_model.bin\n",
      "Image processor saved in ./vit-base-384/checkpoint-6/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "Train Accuracy: 0.995205479452055  Train Loss: 0.013532756306172714  Test Accuracy: 0.9829234972677598  Test Loss: 0.06688504409403023\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09adbedeb074c9fbf48dbef382bb1a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a2b5b620734c2c88b8f3f6462b2512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./vit-base-384/checkpoint-7/config.json\n",
      "Model weights saved in ./vit-base-384/checkpoint-7/pytorch_model.bin\n",
      "Image processor saved in ./vit-base-384/checkpoint-7/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\n",
      "Train Accuracy: 0.9990867579908678  Train Loss: 0.004718294461335599  Test Accuracy: 0.9856557377049184  Test Loss: 0.06850345887958939\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec36243d0b741ea9151aaf72289ae35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ebe33508e6d475d8b35e7d694e0fb67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./vit-base-384/checkpoint-8/config.json\n",
      "Model weights saved in ./vit-base-384/checkpoint-8/pytorch_model.bin\n",
      "Image processor saved in ./vit-base-384/checkpoint-8/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:\n",
      "Train Accuracy: 0.9972602739726029  Train Loss: 0.007451145804167923  Test Accuracy: 0.9829234972677598  Test Loss: 0.07680878962734644\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29fff82639a54933b6a39509ff04fd0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6392d9f13b64fc3bc7284e8aed7bce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./vit-base-384/checkpoint-9/config.json\n",
      "Model weights saved in ./vit-base-384/checkpoint-9/pytorch_model.bin\n",
      "Image processor saved in ./vit-base-384/checkpoint-9/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:\n",
      "Train Accuracy: 0.9972602739726031  Train Loss: 0.00807558549292717  Test Accuracy: 0.9808743169398911  Test Loss: 0.07718623701275198\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750e4458831d4fb6b7573254d2b071aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8617ad04064464bd2797db1b68e8c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./vit-base-384/checkpoint-10/config.json\n",
      "Model weights saved in ./vit-base-384/checkpoint-10/pytorch_model.bin\n",
      "Image processor saved in ./vit-base-384/checkpoint-10/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:\n",
      "Train Accuracy: 0.9995433789954339  Train Loss: 0.0011769281213611527  Test Accuracy: 0.9795081967213121  Test Loss: 0.10447689523981599\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "\n",
    "train_accuracy=[]\n",
    "test_accuracy=[]\n",
    "train_loss=[]\n",
    "test_loss=[]\n",
    "\n",
    "for i in tqdm(range(epochs)):\n",
    "    print(\"Training:\")\n",
    "    model.train()\n",
    "    \n",
    "    #Defining accuracy and loss for train and test data\n",
    "    temp_train_accuracy=[]\n",
    "    temp_test_accuracy=[]\n",
    "    temp_train_loss=[]\n",
    "    temp_test_loss=[]\n",
    "\n",
    "    net_train_accuracy=0\n",
    "    net_test_accuracy=0\n",
    "    net_train_loss=0\n",
    "    net_test_loss=0\n",
    "    \n",
    "    with tqdm(total=len(train_dataset_loader)) as pbar:\n",
    "        for batch in train_dataset_loader:\n",
    "                x=batch[\"pixel_values\"].to(device)\n",
    "                y=batch[\"labels\"].to(device)\n",
    "    \n",
    "                #Calculating model output\n",
    "                result=model(pixel_values=x,labels=y)\n",
    "                logits=result.logits\n",
    "    \n",
    "                #Reseting any old gradient values\n",
    "                optimizer.zero_grad()\n",
    "                loss=result.loss\n",
    "\n",
    "            \n",
    "                #Track of metrics        \n",
    "                accuracy_train=accuracy_fn(logits,y)\n",
    "                temp_train_accuracy.append(accuracy_train)\n",
    "                temp_train_loss.append(loss.item())\n",
    "    \n",
    "                #Back Propogation\n",
    "                loss.backward()\n",
    "            \n",
    "                #Update Parameters\n",
    "                optimizer.step()\n",
    "            \n",
    "                #Progress Bar Update\n",
    "                pbar.update(1)\n",
    "        pbar.close()\n",
    "    #Tensorboard & Metrics for the dataset\n",
    "    net_train_accuracy=sum(temp_train_accuracy)/len(temp_train_accuracy)\n",
    "    net_train_loss=sum(temp_train_loss)/len(temp_train_loss)\n",
    "    train_accuracy.append(net_train_accuracy)\n",
    "    train_loss.append(net_train_loss)\n",
    "    writer.add_scalar(\"Train Accuracy\",net_train_accuracy,i)\n",
    "    writer.add_scalar(\"Train Loss\",net_train_loss,i)\n",
    "\n",
    "    #Evaluation\n",
    "    print(\"Testing:\")\n",
    "    model.eval()\n",
    "\n",
    "    with tqdm(total=len(valid_dataset_loader)) as pbar2:\n",
    "        for batch in valid_dataset_loader:\n",
    "            x=batch[\"pixel_values\"].to(device)\n",
    "            y=batch[\"labels\"].to(device)\n",
    "            \n",
    "            #Setting inference mode\n",
    "            with torch.inference_mode():\n",
    "                result=model(pixel_values=x,labels=y)\n",
    "                logits=result.logits\n",
    "                loss=result.loss\n",
    "\n",
    "                #Track of metrics\n",
    "                accuracy_test=accuracy_fn(logits,y)\n",
    "                temp_test_accuracy.append(accuracy_test)\n",
    "                temp_test_loss.append(loss.item())\n",
    "\n",
    "                #Progress Bar Update\n",
    "                pbar2.update(1)\n",
    "        pbar2.close()\n",
    "\n",
    "    #Tensorboard & Metrics for the dataset\n",
    "    net_test_accuracy=sum(temp_test_accuracy)/len(temp_test_accuracy)\n",
    "    net_test_loss=sum(temp_test_loss)/len(temp_test_loss)\n",
    "    test_accuracy.append(net_test_accuracy)\n",
    "    test_loss.append(net_test_loss)\n",
    "    writer.add_scalar(\"Test Accuracy\",net_test_accuracy,i)\n",
    "    writer.add_scalar(\"Test Loss\",net_test_loss,i)\n",
    "\n",
    "    #Saving the model\n",
    "    model.save_pretrained(f\"./vit-base-384/checkpoint-{i+1}\")\n",
    "    image_processor.save_pretrained(f\"./vit-base-384/checkpoint-{i+1}\")\n",
    "\n",
    "    print(f\"Epoch {i+1}:\\nTrain Accuracy: {net_train_accuracy}  Train Loss: {net_train_loss}  Test Accuracy: {net_test_accuracy}  Test Loss: {net_test_loss}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

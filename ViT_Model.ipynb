{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c15c1c7f-c830-4acf-922d-f26b832d0664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.0+cu121 with CUDA 1201 (you have 2.0.1+cu117)\n",
      "    Python  3.9.18 (you have 3.9.17)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from transformers import *\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torchinfo import summary\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50876b6-87da-467d-8ab7-2379d224e3e1",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e4f22a-4665-449e-a9a1-79f891e6fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(examples):\n",
    "  inputs = image_processor([img.convert(\"RGB\") for img in examples[\"image\"]], return_tensors=\"pt\")\n",
    "  inputs[\"labels\"] = examples[\"label\"]\n",
    "  return inputs\n",
    "\n",
    "def collate_fn(batch):\n",
    "  return {\n",
    "      \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
    "      \"labels\": torch.tensor([x[\"labels\"] for x in batch]),\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ed81720-8a58-45b2-8f19-37d29e2a7cf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at /home/moose/.cache/huggingface/hub/models--google--vit-base-patch16-384/snapshots/2960116e809e2fca84146dbb240289aee7db4827/preprocessor_config.json\n",
      "size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}), got 384. Converted to {'height': 384, 'width': 384}.\n",
      "Image processor ViTImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 384,\n",
      "    \"width\": 384\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38840cf2c3c4436b3c36c98c4375c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/4380 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ba4bb31fee4b80b335b55876348f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881cfe53506542d1a282048b342c6979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/4380 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f0c8c88d0241cf93e63c080a3d7f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569ef740c29b4737b77d927815e20b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6201211162448fb6c8b1fc2600e3c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63b2730f23d4f03ba701ed80db9e565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c04e640e1a724d63a855400d2e9019d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ebd7c88434a40e2bca7b391f34d1fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e91ecf703c436ab931897fab898225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"google/vit-base-patch16-384\"\n",
    "batch_size = 6\n",
    "cpu_count=multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "image_processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "ds = load_dataset('imagefolder', data_dir='./data/')\n",
    "\n",
    "labels = ds[\"train\"].features[\"label\"].names\n",
    "\n",
    "\n",
    "dataset = ds.with_transform(transform)\n",
    "\n",
    "train_dataset_loader = torch.utils.data.DataLoader(dataset[\"train\"], collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\n",
    "valid_dataset_loader = torch.utils.data.DataLoader(dataset[\"test\"], collate_fn=collate_fn, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840f4076-1faa-4ccc-b98f-c652ece789a6",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cac33c31-2a45-4e66-b036-5a563206f18c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/moose/.cache/huggingface/hub/models--google--vit-base-patch16-384/snapshots/2960116e809e2fca84146dbb240289aee7db4827/config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-base-patch16-384\",\n",
      "  \"architectures\": [\n",
      "    \"ViTForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NORMAL\",\n",
      "    \"1\": \"PNEUMONIA\"\n",
      "  },\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"NORMAL\": \"0\",\n",
      "    \"PNEUMONIA\": \"1\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/moose/.cache/huggingface/hub/models--google--vit-base-patch16-384/snapshots/2960116e809e2fca84146dbb240289aee7db4827/model.safetensors\n",
      "All model checkpoint weights were used when initializing ViTForImageClassification.\n",
      "\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-384 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)},\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fe718b-8a51-49cb-a72c-5735ac890eb1",
   "metadata": {},
   "source": [
    "### Optimizer and accuracy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb327302-863a-4cee-918c-4a530c39a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "log_dir = \"./ViT_base_384_tensorboard/\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "def accuracy_fn(logits,true):\n",
    "    return torch.eq(torch.argmax(torch.softmax(logits,dim=1),dim=1).squeeze(),true).sum().item()/len(logits)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "920d9786-0c53-4a6b-a1a6-73b46201fdff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb14f955d34949b9bb21cef30ed22fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337f00386559489b94089c6cd6fc4c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd85d3d995c480a87a04a864d68ccda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train Accuracy: 0.944292237442923  Train Loss: 0.14255950639840878  Test Accuracy: 0.9672131147540985  Test Loss: 0.0889992063295394\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d61af0d37124294a5c492c4913eec5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20e25dabc8040f794ee92cad6f021d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "Train Accuracy: 0.9810502283105027  Train Loss: 0.06036335184826509  Test Accuracy: 0.9781420765027327  Test Loss: 0.06674702437217614\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c1cdea26484aaf86c535f493a745d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d8a4b33f2e43149fa2a2abebdcba18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "Train Accuracy: 0.9865296803652975  Train Loss: 0.0356926561939436  Test Accuracy: 0.9760928961748637  Test Loss: 0.07909315865719214\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3354817af374c67997e94ffd27cabd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b31a185a857494aaa27526845350be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "Train Accuracy: 0.9952054794520548  Train Loss: 0.01551557696751444  Test Accuracy: 0.9801912568306013  Test Loss: 0.06725753904513232\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062da0543af34aeba9f68a1741df1736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1578c7b5ed84ef58ca381583b7df75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "Train Accuracy: 0.9956621004566214  Train Loss: 0.012494350239417331  Test Accuracy: 0.974043715846995  Test Loss: 0.08017328407789986\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606fa4b684ae4054a5bbcb8e60c1ac77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d7c9a0518a4be9996897a74d050c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "Train Accuracy: 0.9970319634703196  Train Loss: 0.009982734212009172  Test Accuracy: 0.9760928961748637  Test Loss: 0.09624579218244121\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591625cf8271479eb5c3b4346dd9f001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753d78cc49de45309c35eab00b107625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\n",
      "Train Accuracy: 0.9977168949771689  Train Loss: 0.006494913694270558  Test Accuracy: 0.9781420765027325  Test Loss: 0.08522418619556492\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff11feedd63842ce936c7a6f218a97c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m temp_train_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#Back Propogation\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#Update Parameters\u001b[39;00m\n\u001b[1;32m     46\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "\n",
    "train_accuracy=[]\n",
    "test_accuracy=[]\n",
    "train_loss=[]\n",
    "test_loss=[]\n",
    "\n",
    "for i in tqdm(range(epochs)):\n",
    "    print(\"Training:\")\n",
    "    model.train()\n",
    "    \n",
    "    #Defining accuracy and loss for train and test data\n",
    "    temp_train_accuracy=[]\n",
    "    temp_test_accuracy=[]\n",
    "    temp_train_loss=[]\n",
    "    temp_test_loss=[]\n",
    "\n",
    "    net_train_accuracy=0\n",
    "    net_test_accuracy=0\n",
    "    net_train_loss=0\n",
    "    net_test_loss=0\n",
    "    \n",
    "    with tqdm(total=len(train_dataset_loader)) as pbar:\n",
    "        for batch in train_dataset_loader:\n",
    "                x=batch[\"pixel_values\"].to(device)\n",
    "                y=batch[\"labels\"].to(device)\n",
    "    \n",
    "                #Calculating model output\n",
    "                result=model(pixel_values=x,labels=y)\n",
    "                logits=result.logits\n",
    "    \n",
    "                #Reseting any old gradient values\n",
    "                optimizer.zero_grad()\n",
    "                loss=result.loss\n",
    "\n",
    "            \n",
    "                #Track of metrics        \n",
    "                accuracy_train=accuracy_fn(logits,y)\n",
    "                temp_train_accuracy.append(accuracy_train)\n",
    "                temp_train_loss.append(loss.item())\n",
    "    \n",
    "                #Back Propogation\n",
    "                loss.backward()\n",
    "            \n",
    "                #Update Parameters\n",
    "                optimizer.step()\n",
    "            \n",
    "                #Progress Bar Update\n",
    "                pbar.update(1)\n",
    "        pbar.close()\n",
    "    #Tensorboard & Metrics for the dataset\n",
    "    net_train_accuracy=sum(temp_train_accuracy)/len(temp_train_accuracy)\n",
    "    net_train_loss=sum(temp_train_loss)/len(temp_train_loss)\n",
    "    train_accuracy.append(net_train_accuracy)\n",
    "    train_loss.append(net_train_loss)\n",
    "    writer.add_scalar(\"Train Accuracy\",net_train_accuracy,i)\n",
    "    writer.add_scalar(\"Train Loss\",net_train_loss,i)\n",
    "\n",
    "    #Evaluation\n",
    "    print(\"Testing:\")\n",
    "    model.eval()\n",
    "\n",
    "    with tqdm(total=len(valid_dataset_loader)) as pbar2:\n",
    "        for batch in valid_dataset_loader:\n",
    "            x=batch[\"pixel_values\"].to(device)\n",
    "            y=batch[\"labels\"].to(device)\n",
    "            \n",
    "            #Setting inference mode\n",
    "            with torch.inference_mode():\n",
    "                result=model(pixel_values=x,labels=y)\n",
    "                logits=result.logits\n",
    "                loss=result.loss\n",
    "\n",
    "                #Track of metrics\n",
    "                accuracy_test=accuracy_fn(logits,y)\n",
    "                temp_test_accuracy.append(accuracy_test)\n",
    "                temp_test_loss.append(loss.item())\n",
    "\n",
    "                #Progress Bar Update\n",
    "                pbar2.update(1)\n",
    "        pbar2.close()\n",
    "\n",
    "    #Tensorboard & Metrics for the dataset\n",
    "    net_test_accuracy=sum(temp_test_accuracy)/len(temp_test_accuracy)\n",
    "    net_test_loss=sum(temp_test_loss)/len(temp_test_loss)\n",
    "    test_accuracy.append(net_test_accuracy)\n",
    "    test_loss.append(net_test_loss)\n",
    "    writer.add_scalar(\"Test Accuracy\",net_test_accuracy,i)\n",
    "    writer.add_scalar(\"Test Loss\",net_test_loss,i)\n",
    "\n",
    "    #Saving the model\n",
    "    #model.save_pretrained(f\"./vit-base-384/checkpoint-{i+1}\")\n",
    "    #image_processor.save_pretrained(f\"./vit-base-384/checkpoint-{i+1}\")\n",
    "\n",
    "    print(f\"Epoch {i+1}:\\nTrain Accuracy: {net_train_accuracy}  Train Loss: {net_train_loss}  Test Accuracy: {net_test_accuracy}  Test Loss: {net_test_loss}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2442d48e-e8f4-498f-a074-9dff6267d2ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

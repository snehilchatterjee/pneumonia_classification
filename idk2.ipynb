{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa28ea2c-ac78-444e-b68f-70bf3cc8de3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.0+cu121 with CUDA 1201 (you have 2.0.1+cu117)\n",
      "    Python  3.9.18 (you have 3.9.17)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import torchvision\n",
    "import timm\n",
    "import os\n",
    "from transformers import *\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torchinfo import summary\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "from torch.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edba117d-d4c7-42d3-983b-cc2dd584084f",
   "metadata": {},
   "source": [
    "### Loading in data and pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "489bd0a5-59d3-4b02-a852-8485c9f7aed5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vit_base_patch8_224.augreg2_in21k_ft_in1k',\n",
       " 'vit_base_patch8_224.augreg_in21k',\n",
       " 'vit_base_patch8_224.augreg_in21k_ft_in1k',\n",
       " 'vit_base_patch8_224.dino',\n",
       " 'vit_base_patch14_dinov2.lvd142m',\n",
       " 'vit_base_patch16_224.augreg2_in21k_ft_in1k',\n",
       " 'vit_base_patch16_224.augreg_in1k',\n",
       " 'vit_base_patch16_224.augreg_in21k',\n",
       " 'vit_base_patch16_224.augreg_in21k_ft_in1k',\n",
       " 'vit_base_patch16_224.dino',\n",
       " 'vit_base_patch16_224.mae',\n",
       " 'vit_base_patch16_224.orig_in21k_ft_in1k',\n",
       " 'vit_base_patch16_224.sam_in1k',\n",
       " 'vit_base_patch16_224_miil.in21k',\n",
       " 'vit_base_patch16_224_miil.in21k_ft_in1k',\n",
       " 'vit_base_patch16_384.augreg_in1k',\n",
       " 'vit_base_patch16_384.augreg_in21k_ft_in1k',\n",
       " 'vit_base_patch16_384.orig_in21k_ft_in1k',\n",
       " 'vit_base_patch16_clip_224.datacompxl',\n",
       " 'vit_base_patch16_clip_224.laion2b',\n",
       " 'vit_base_patch16_clip_224.laion2b_ft_in1k',\n",
       " 'vit_base_patch16_clip_224.laion2b_ft_in12k',\n",
       " 'vit_base_patch16_clip_224.laion2b_ft_in12k_in1k',\n",
       " 'vit_base_patch16_clip_224.openai',\n",
       " 'vit_base_patch16_clip_224.openai_ft_in1k',\n",
       " 'vit_base_patch16_clip_224.openai_ft_in12k',\n",
       " 'vit_base_patch16_clip_224.openai_ft_in12k_in1k',\n",
       " 'vit_base_patch16_clip_384.laion2b_ft_in1k',\n",
       " 'vit_base_patch16_clip_384.laion2b_ft_in12k_in1k',\n",
       " 'vit_base_patch16_clip_384.openai_ft_in1k',\n",
       " 'vit_base_patch16_clip_384.openai_ft_in12k_in1k',\n",
       " 'vit_base_patch16_rpn_224.sw_in1k',\n",
       " 'vit_base_patch16_siglip_224.webli',\n",
       " 'vit_base_patch16_siglip_256.webli',\n",
       " 'vit_base_patch16_siglip_384.webli',\n",
       " 'vit_base_patch16_siglip_512.webli',\n",
       " 'vit_base_patch32_224.augreg_in1k',\n",
       " 'vit_base_patch32_224.augreg_in21k',\n",
       " 'vit_base_patch32_224.augreg_in21k_ft_in1k',\n",
       " 'vit_base_patch32_224.sam_in1k',\n",
       " 'vit_base_patch32_384.augreg_in1k',\n",
       " 'vit_base_patch32_384.augreg_in21k_ft_in1k',\n",
       " 'vit_base_patch32_clip_224.laion2b',\n",
       " 'vit_base_patch32_clip_224.laion2b_ft_in1k',\n",
       " 'vit_base_patch32_clip_224.laion2b_ft_in12k_in1k',\n",
       " 'vit_base_patch32_clip_224.openai',\n",
       " 'vit_base_patch32_clip_224.openai_ft_in1k',\n",
       " 'vit_base_patch32_clip_384.laion2b_ft_in12k_in1k',\n",
       " 'vit_base_patch32_clip_384.openai_ft_in12k_in1k',\n",
       " 'vit_base_patch32_clip_448.laion2b_ft_in12k_in1k',\n",
       " 'vit_base_r50_s16_224.orig_in21k',\n",
       " 'vit_base_r50_s16_384.orig_in21k_ft_in1k',\n",
       " 'vit_giant_patch14_clip_224.laion2b',\n",
       " 'vit_giant_patch14_dinov2.lvd142m',\n",
       " 'vit_giant_patch16_gap_224.in22k_ijepa',\n",
       " 'vit_gigantic_patch14_clip_224.laion2b',\n",
       " 'vit_huge_patch14_224.mae',\n",
       " 'vit_huge_patch14_224.orig_in21k',\n",
       " 'vit_huge_patch14_clip_224.laion2b',\n",
       " 'vit_huge_patch14_clip_224.laion2b_ft_in1k',\n",
       " 'vit_huge_patch14_clip_224.laion2b_ft_in12k',\n",
       " 'vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k',\n",
       " 'vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k',\n",
       " 'vit_huge_patch14_gap_224.in1k_ijepa',\n",
       " 'vit_huge_patch14_gap_224.in22k_ijepa',\n",
       " 'vit_huge_patch16_gap_448.in1k_ijepa',\n",
       " 'vit_large_patch14_clip_224.datacompxl',\n",
       " 'vit_large_patch14_clip_224.laion2b',\n",
       " 'vit_large_patch14_clip_224.laion2b_ft_in1k',\n",
       " 'vit_large_patch14_clip_224.laion2b_ft_in12k',\n",
       " 'vit_large_patch14_clip_224.laion2b_ft_in12k_in1k',\n",
       " 'vit_large_patch14_clip_224.openai',\n",
       " 'vit_large_patch14_clip_224.openai_ft_in1k',\n",
       " 'vit_large_patch14_clip_224.openai_ft_in12k',\n",
       " 'vit_large_patch14_clip_224.openai_ft_in12k_in1k',\n",
       " 'vit_large_patch14_clip_336.laion2b_ft_in1k',\n",
       " 'vit_large_patch14_clip_336.laion2b_ft_in12k_in1k',\n",
       " 'vit_large_patch14_clip_336.openai',\n",
       " 'vit_large_patch14_clip_336.openai_ft_in12k_in1k',\n",
       " 'vit_large_patch14_dinov2.lvd142m',\n",
       " 'vit_large_patch16_224.augreg_in21k',\n",
       " 'vit_large_patch16_224.augreg_in21k_ft_in1k',\n",
       " 'vit_large_patch16_224.mae',\n",
       " 'vit_large_patch16_384.augreg_in21k_ft_in1k',\n",
       " 'vit_large_patch16_siglip_256.webli',\n",
       " 'vit_large_patch16_siglip_384.webli',\n",
       " 'vit_large_patch32_224.orig_in21k',\n",
       " 'vit_large_patch32_384.orig_in21k_ft_in1k',\n",
       " 'vit_large_r50_s32_224.augreg_in21k',\n",
       " 'vit_large_r50_s32_224.augreg_in21k_ft_in1k',\n",
       " 'vit_large_r50_s32_384.augreg_in21k_ft_in1k',\n",
       " 'vit_medium_patch16_gap_240.sw_in12k',\n",
       " 'vit_medium_patch16_gap_256.sw_in12k_ft_in1k',\n",
       " 'vit_medium_patch16_gap_384.sw_in12k_ft_in1k',\n",
       " 'vit_relpos_base_patch16_224.sw_in1k',\n",
       " 'vit_relpos_base_patch16_clsgap_224.sw_in1k',\n",
       " 'vit_relpos_base_patch32_plus_rpn_256.sw_in1k',\n",
       " 'vit_relpos_medium_patch16_224.sw_in1k',\n",
       " 'vit_relpos_medium_patch16_cls_224.sw_in1k',\n",
       " 'vit_relpos_medium_patch16_rpn_224.sw_in1k',\n",
       " 'vit_relpos_small_patch16_224.sw_in1k',\n",
       " 'vit_small_patch8_224.dino',\n",
       " 'vit_small_patch14_dinov2.lvd142m',\n",
       " 'vit_small_patch16_224.augreg_in1k',\n",
       " 'vit_small_patch16_224.augreg_in21k',\n",
       " 'vit_small_patch16_224.augreg_in21k_ft_in1k',\n",
       " 'vit_small_patch16_224.dino',\n",
       " 'vit_small_patch16_384.augreg_in1k',\n",
       " 'vit_small_patch16_384.augreg_in21k_ft_in1k',\n",
       " 'vit_small_patch32_224.augreg_in21k',\n",
       " 'vit_small_patch32_224.augreg_in21k_ft_in1k',\n",
       " 'vit_small_patch32_384.augreg_in21k_ft_in1k',\n",
       " 'vit_small_r26_s32_224.augreg_in21k',\n",
       " 'vit_small_r26_s32_224.augreg_in21k_ft_in1k',\n",
       " 'vit_small_r26_s32_384.augreg_in21k_ft_in1k',\n",
       " 'vit_so400m_patch14_siglip_224.webli',\n",
       " 'vit_so400m_patch14_siglip_384.webli',\n",
       " 'vit_srelpos_medium_patch16_224.sw_in1k',\n",
       " 'vit_srelpos_small_patch16_224.sw_in1k',\n",
       " 'vit_tiny_patch16_224.augreg_in21k',\n",
       " 'vit_tiny_patch16_224.augreg_in21k_ft_in1k',\n",
       " 'vit_tiny_patch16_384.augreg_in21k_ft_in1k',\n",
       " 'vit_tiny_r_s16_p8_224.augreg_in21k',\n",
       " 'vit_tiny_r_s16_p8_224.augreg_in21k_ft_in1k',\n",
       " 'vit_tiny_r_s16_p8_384.augreg_in21k_ft_in1k']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.list_models(\"vit*\",pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d856b9a6-b134-442e-8248-014e110512e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Invalid pretrained tag (webli') for vit_base_patch16_siglip_512.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m cpu_count\u001b[38;5;241m=\u001b[39mmultiprocessing\u001b[38;5;241m.\u001b[39mcpu_count()\n\u001b[1;32m      3\u001b[0m model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvit_base_patch16_siglip_512.webli\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtimm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m data_config \u001b[38;5;241m=\u001b[39m timm\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mresolve_model_data_config(model)\n\u001b[1;32m      8\u001b[0m transforms \u001b[38;5;241m=\u001b[39m timm\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcreate_transform(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata_config, is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/timm/models/_factory.py:114\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model_name, pretrained, pretrained_cfg, pretrained_cfg_overlay, checkpoint_path, scriptable, exportable, no_jit, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m create_fn \u001b[38;5;241m=\u001b[39m model_entrypoint(model_name)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_layer_config(scriptable\u001b[38;5;241m=\u001b[39mscriptable, exportable\u001b[38;5;241m=\u001b[39mexportable, no_jit\u001b[38;5;241m=\u001b[39mno_jit):\n\u001b[0;32m--> 114\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_cfg_overlay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_cfg_overlay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_path:\n\u001b[1;32m    122\u001b[0m     load_checkpoint(model, checkpoint_path)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/timm/models/vision_transformer.py:2262\u001b[0m, in \u001b[0;36mvit_base_patch16_siglip_512\u001b[0;34m(pretrained, **kwargs)\u001b[0m\n\u001b[1;32m   2257\u001b[0m \u001b[38;5;129m@register_model\u001b[39m\n\u001b[1;32m   2258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvit_base_patch16_siglip_512\u001b[39m(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VisionTransformer:\n\u001b[1;32m   2259\u001b[0m     model_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m   2260\u001b[0m         patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, class_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, global_pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   2261\u001b[0m     )\n\u001b[0;32m-> 2262\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_create_vision_transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvit_base_patch16_siglip_512\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/timm/models/vision_transformer.py:1611\u001b[0m, in \u001b[0;36m_create_vision_transformer\u001b[0;34m(variant, pretrained, **kwargs)\u001b[0m\n\u001b[1;32m   1608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msiglip\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m variant \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_pool\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1609\u001b[0m     strict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1611\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_model_with_cfg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mVisionTransformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_filter_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_filter_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_strict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/timm/models/_builder.py:365\u001b[0m, in \u001b[0;36mbuild_model_with_cfg\u001b[0;34m(model_cls, variant, pretrained, pretrained_cfg, pretrained_cfg_overlay, model_cfg, feature_cfg, pretrained_strict, pretrained_filter_fn, kwargs_filter, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m feature_cfg \u001b[38;5;241m=\u001b[39m feature_cfg \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# resolve and update model pretrained config and model kwargs\u001b[39;00m\n\u001b[0;32m--> 365\u001b[0m pretrained_cfg \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_pretrained_cfg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_cfg_overlay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_cfg_overlay\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# FIXME converting back to dict, PretrainedCfg use should be propagated further, but not into model\u001b[39;00m\n\u001b[1;32m    372\u001b[0m pretrained_cfg \u001b[38;5;241m=\u001b[39m pretrained_cfg\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/timm/models/_builder.py:311\u001b[0m, in \u001b[0;36mresolve_pretrained_cfg\u001b[0;34m(variant, pretrained_cfg, pretrained_cfg_overlay)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pretrained_tag:\n\u001b[1;32m    310\u001b[0m         model_with_tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([variant, pretrained_tag])\n\u001b[0;32m--> 311\u001b[0m     pretrained_cfg \u001b[38;5;241m=\u001b[39m \u001b[43mget_pretrained_cfg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_with_tag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pretrained_cfg:\n\u001b[1;32m    314\u001b[0m     _logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo pretrained configuration specified for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_with_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m model. Using a default.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please add a config to the model pretrained_cfg registry or pass explicitly.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/timm/models/_registry.py:317\u001b[0m, in \u001b[0;36mget_pretrained_cfg\u001b[0;34m(model_name, allow_unregistered)\u001b[0m\n\u001b[1;32m    314\u001b[0m arch_name, tag \u001b[38;5;241m=\u001b[39m split_model_name_tag(model_name)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arch_name \u001b[38;5;129;01min\u001b[39;00m _model_default_cfgs:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;66;03m# if model arch exists, but the tag is wrong, error out\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid pretrained tag (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00march_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_unregistered:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# if model arch doesn't exist, it has no pretrained_cfg registered, allow a default to be created\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Invalid pretrained tag (webli') for vit_base_patch16_siglip_512."
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "cpu_count=multiprocessing.cpu_count()\n",
    "model_name=\"vit_base_patch16_siglip_512.webli\"\n",
    "\n",
    "model = timm.create_model(model_name, pretrained=True)\n",
    "\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n",
    "transform_data=torchvision.transforms.Compose([torchvision.transforms.Resize(size=(224,224)),\n",
    "                                                torchvision.transforms.ToTensor()])\n",
    "\n",
    "train_data=torchvision.datasets.ImageFolder(\"./data/train\",transform=transforms)\n",
    "test_data=torchvision.datasets.ImageFolder(\"./data/test\",transform=transforms)\n",
    "\n",
    "train_loader=torch.utils.data.DataLoader(train_data,shuffle=True,batch_size=batch_size,num_workers=cpu_count)\n",
    "test_loader=torch.utils.data.DataLoader(test_data,shuffle=True,batch_size=batch_size,num_workers=cpu_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0acec8-1366-4384-978a-ed496227dceb",
   "metadata": {},
   "source": [
    "##### Freezing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5659880b-d3de-407d-a21e-0c4c26010804",
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for param in model.parameters(): #153 params\n",
    "    if(count>=120):\n",
    "        break\n",
    "    param.requires_grad=False\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb2f8ee-163f-47a0-8309-ecc9603374a2",
   "metadata": {},
   "source": [
    "##### Model Modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490c6381-8d61-4d86-8985-2cd9cab43279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the classification part\n",
    "model.head=nn.Linear(in_features=model.head.in_features,out_features=1024)\n",
    "\n",
    "class modified_vit(nn.Module):\n",
    "    def __init__(self,model):\n",
    "        super().__init__()\n",
    "        self.model=model\n",
    "        self.sequential=nn.Sequential(nn.Linear(in_features=1024,out_features=2048),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(in_features=2048,out_features=512),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(in_features=512,out_features=256),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(in_features=256,out_features=128),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(in_features=128,out_features=2))\n",
    "    def forward(self,x):\n",
    "        return self.sequential(self.model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f130ac12-876e-4507-8bac-8faae14ac052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=modified_vit(model)\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6290eb6f-24a4-4e01-948d-2a1ab0ca2b11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407a09f5-52a7-4bb8-8e91-42667d095112",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54de109f-088e-427b-a5cf-8ead5d4cb6a7",
   "metadata": {},
   "source": [
    "### Visualizing Transformed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404d9f40-1338-492a-a2c1-1da63f9f904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "for i in range(1,10):\n",
    "    data=next(iter(train_loader))\n",
    "    rand_ind=torch.randint(0,batch_size-1,size=(1,)).item()\n",
    "    label=data[1][rand_ind]\n",
    "    image=data[0][rand_ind].permute(1,2,0)\n",
    "    plt.title(train_data.classes[label.item()])\n",
    "    plt.subplot(3,3,i)\n",
    "    plt.axis(False)\n",
    "    plt.imshow(image)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a518d79-6a74-458a-9181-18ba3f1c8071",
   "metadata": {},
   "source": [
    "### Optimizer and Loss and Logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a865d6-c9eb-4c64-ae6b-5ec9aa8300f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.AdamW(model.parameters())\n",
    "loss_fn=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccec224-32ee-44b5-8e50-790b6fb2f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./\"+model_name+\"feature_extractor_tensorboard\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "def accuracy_fn(logits,true):\n",
    "    return torch.eq(torch.argmax(torch.softmax(logits,dim=1),dim=1).squeeze(),true).sum().item()/len(logits)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b165be-58a5-491f-8c86-69f0e642a3fe",
   "metadata": {},
   "source": [
    "### Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e029afe9-c46a-49be-acc9-ae2ba6845ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    x,y=next(iter(train_loader))\n",
    "    x=x.to(device)\n",
    "    y=y.to(device)\n",
    "    logits=model(x)\n",
    "    print(logits)\n",
    "    print(loss_fn(logits.squeeze(),y))\n",
    "    print(accuracy_fn(logits,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf8b13-ea45-4f50-a4a4-f88786397ba6",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9275cf-74e3-492c-a39e-8118a764ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "\n",
    "train_accuracy=[]\n",
    "test_accuracy=[]\n",
    "train_loss=[]\n",
    "test_loss=[]\n",
    "\n",
    "for i in tqdm(range(epochs)):\n",
    "    print(\"Training:\")\n",
    "    model.train()\n",
    "    \n",
    "    #Defining accuracy and loss for train and test data\n",
    "    temp_train_accuracy=[]\n",
    "    temp_test_accuracy=[]\n",
    "    temp_train_loss=[]\n",
    "    temp_test_loss=[]\n",
    "\n",
    "    net_train_accuracy=0\n",
    "    net_test_accuracy=0\n",
    "    net_train_loss=0\n",
    "    net_test_loss=0\n",
    "    \n",
    "    with tqdm(total=len(train_loader)) as pbar:\n",
    "        for x,y in train_loader:\n",
    "                x=x.to(device)\n",
    "                y=y.to(device)\n",
    "    \n",
    "                #Calculating model output\n",
    "                logits=model(x)\n",
    "    \n",
    "                #Reseting any old gradient values\n",
    "                optimizer.zero_grad()\n",
    "                loss=loss_fn(logits.squeeze(),y)\n",
    "\n",
    "            \n",
    "                #Track of metrics        \n",
    "                accuracy_train=accuracy_fn(logits.type(torch.float32),y)\n",
    "                temp_train_accuracy.append(accuracy_train)\n",
    "                temp_train_loss.append(loss.item())\n",
    "    \n",
    "                #Back Propogation\n",
    "                loss.backward()\n",
    "            \n",
    "                #Update Parameters\n",
    "                optimizer.step()\n",
    "            \n",
    "                #Progress Bar Update\n",
    "                pbar.update(1)\n",
    "        pbar.close()\n",
    "    #Tensorboard & Metrics for the dataset\n",
    "    net_train_accuracy=sum(temp_train_accuracy)/len(temp_train_accuracy)\n",
    "    net_train_loss=sum(temp_train_loss)/len(temp_train_loss)\n",
    "    train_accuracy.append(net_train_accuracy)\n",
    "    train_loss.append(net_train_loss)\n",
    "    writer.add_scalar(\"Train Accuracy\",net_train_accuracy,i)\n",
    "    writer.add_scalar(\"Train Loss\",net_train_loss,i)\n",
    "\n",
    "    #Evaluation\n",
    "    print(\"Testing:\")\n",
    "    model.eval()\n",
    "\n",
    "    with tqdm(total=len(test_loader)) as pbar2:\n",
    "        for x,y in test_loader:\n",
    "            x=x.to(device)\n",
    "            y=y.to(device)\n",
    "            \n",
    "            #Setting inference mode\n",
    "            with torch.inference_mode():\n",
    "                logits=model(x)\n",
    "                loss=loss_fn(logits.squeeze().type(torch.float32),y)\n",
    "\n",
    "                #Track of metrics\n",
    "                accuracy_test=accuracy_fn(logits,y)\n",
    "                temp_test_accuracy.append(accuracy_test)\n",
    "                temp_test_loss.append(loss.item())\n",
    "\n",
    "                #Progress Bar Update\n",
    "                pbar2.update(1)\n",
    "        pbar2.close()\n",
    "\n",
    "    #Tensorboard & Metrics for the dataset\n",
    "    net_test_accuracy=sum(temp_test_accuracy)/len(temp_test_accuracy)\n",
    "    net_test_loss=sum(temp_test_loss)/len(temp_test_loss)\n",
    "    test_accuracy.append(net_test_accuracy)\n",
    "    test_loss.append(net_test_loss)\n",
    "    writer.add_scalar(\"Test Accuracy\",net_test_accuracy,i)\n",
    "    writer.add_scalar(\"Test Loss\",net_test_loss,i)\n",
    "\n",
    "    '''\n",
    "    #Saving the model\n",
    "    try:\n",
    "        os.makedirs(f\"./{model_name}_feature_extractor/\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    torch.save(model.state_dict(),f\"./{model_name}_feature_extractor/checkpoint-{i+1}.pth\")\n",
    "    '''\n",
    "    \n",
    "    print(f\"Epoch {i+1}:\\nTrain Accuracy: {net_train_accuracy}  Train Loss: {net_train_loss}  Test Accuracy: {net_test_accuracy}  Test Loss: {net_test_loss}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67231fcf-4211-4219-b7cb-add0bcd14734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
